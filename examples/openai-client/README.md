# OpenAI Client Demo

A standalone demo application showcasing the `@katechat/ui` package with direct OpenAI-compatible API integration.

## Features

- ğŸ¨ Uses `@katechat/ui` components (`ChatMessagesContainer`, `ChatMessagesList`, `ChatInput`)
- ğŸ”Œ Direct browser connection to OpenAI-compatible APIs (OpenAI, DeepSeek)
- ğŸ’¾ **IndexedDB storage** for persistent chat history (chats and messages stored locally)
- ğŸ“‹ **Multiple chat management** with organized chat list using `sortItemsBySections`
- ğŸ·ï¸ **Automatic chat titles** generated by LLM after first message exchange
- ğŸ“Œ Pin important chats for quick access
- ğŸ“… Chats organized by time sections (Today, Yesterday, Last 7 days, etc.)
- ğŸ” User-provided API key and endpoint configuration
- ğŸ”„ Supports both Chat Completions and Responses APIs
- ğŸ“¡ Built-in CORS proxy for development
- âš¡ Fast development with esbuild and HMR

## Getting Started

### Installation

```bash
npm install
```

### Development

```bash
npm run dev
```

The app will be available at http://localhost:3001

### Production Build

```bash
npm run build
```

The build output will be in the `dist/` folder.

## Configuration

1. Click the **Settings** button in the top right corner
2. Enter your API key
3. Select an API endpoint (OpenAI, Yandex FM, DeepSeek, or custom)
4. Choose API mode:
   - **Chat Completions**: Uses `/chat/completions` endpoint (recommended)
   - **Text Completions**: Uses `/completions` endpoint
5. Select or enter a model name
6. Click **Save Settings**

Your configuration is stored in localStorage.

## Supported API Providers

### OpenAI
- Endpoint: `https://api.openai.com/v1`
- Models: `gpt-4o`, `gpt-4.1-mini`, `gpt-4-turbo`, `gpt-3.5-turbo`

### DeepSeek
- Endpoint: `https://api.deepseek.com/v1`
- Models: `deepseek-chat`, `deepseek-reasoner`

### Custom
- Any OpenAI-compatible API endpoint

## Architecture

- **Frontend**: React 19 + TypeScript + @katechat/ui
- **Styling**: Mantine UI + SCSS modules
- **Build**: esbuild for fast bundling
- **Storage**: IndexedDB (via `idb` library) for persistent local data
- **API Client**: Custom streaming client with SSE support
- **CORS Proxy**: Node.js HTTP proxy for development (avoids CORS issues)

## Project Structure

```
src/
â”œâ”€â”€ index.tsx              # App entry point
â”œâ”€â”€ App.tsx                # Main app component
â”œâ”€â”€ App.scss               # App styles
â”œâ”€â”€ styles.scss            # Global styles
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ChatList.tsx       # Chat list with sections
â”‚   â””â”€â”€ SettingsForm.tsx   # API configuration form
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useChats.ts        # Chat management hook
â”‚   â””â”€â”€ useMessages.ts     # Message management hook
â””â”€â”€ lib/
    â”œâ”€â”€ db.ts              # IndexedDB wrapper
    â””â”€â”€ openai-client.ts   # OpenAI API client with streaming & title generation
```

## How It Works

### Chat Management

1. Click the **Chats** button to open the sidebar
2. View all your chats organized by time sections (using `sortItemsBySections` from `@katechat/ui`)
3. Click **New** to create a new chat
4. Pin important chats for quick access
5. Delete chats you no longer need
6. All chats are stored in IndexedDB and persist across sessions

### Chat Flow

1. User selects a chat or creates a new one
2. User types a message in `ChatInput`
3. Message is added to IndexedDB and the `messages` state
4. `OpenAIClient` sends request to the configured API endpoint
5. Response is streamed back chunk by chunk
6. Messages are updated in real-time in `ChatMessagesContainer`
7. **After the first exchange**, the LLM automatically generates a concise chat title
8. Chat's `updatedAt` timestamp is updated for proper sorting

### IndexedDB Schema

- **chats** store: `{ id, title, createdAt, updatedAt, isPinned, modelName }`
- **messages** store: `{ id, chatId, content, role, modelName, createdAt, updatedAt, ... }`
- Indexed by `chatId` for efficient queries

### CORS Proxy

In development mode, the app uses a proxy server to avoid CORS issues:
- Client makes request to `/proxy/{encoded-url}`
- Proxy forwards request to the actual API
- Response is streamed back with proper CORS headers

In production, you would deploy this behind a proper backend or use the API's CORS configuration.

## License

MIT
